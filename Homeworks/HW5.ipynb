#Problem 1 
import torch
import torch.optim as optim
from matplotlib import pyplot as plt


t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]
t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]
# convert to tensors
t_c = torch.tensor(t_c)
t_u = torch.tensor(t_u)

t_un = t_u * 0.01

# (A)
# Define the models
def model_nonLin(t_u, w1, w2, b) :
    return w2 * t_u ** 2 + w1 * t_u + b

def model_lin(t_u, w, b) :
    return w * t_u + b

# Set up loss function
def loss_fn(t_p, t_c) :
    squared_diffs = (t_p - t_c)**2
    return squared_diffs.mean()

# Set up parameters and optimizers
nonLin_parameters = torch.tensor([1.0, 0.0, 0.0], requires_grad=True) # w0 w1 w2
lin_parameters = torch.tensor([1.0, 0.0], requires_grad=True)
lr = 0.1
nonLinOpt_SGD = optim.SGD([nonLin_parameters], lr=lr)
linOpt_SGD = optim.SGD([lin_parameters], lr=lr)
nonLinOpt_ADAM = optim.Adam([nonLin_parameters], lr=lr)
linOpt_ADAM = optim.Adam([lin_parameters], lr=lr)


# Training Function
def training_nonLin(epochs, opt, params, t_u, t_c) :
    for epoch in range(1, epochs + 1):
        t_p = model_nonLin(t_u, *params)
        loss = loss_fn(t_p, t_c)

        opt.zero_grad()
        loss.backward()
        opt.step()

        if epoch % 500 == 0:
            print('Epoch %d, Loss %f' % (epoch, float(loss)))

    return params

# build autograd loop from notes/textbook for original linear model
def training_lin(epochs, lr, params, t_u, t_c) :
    for epoch in range(1, epochs + 1):
        if params.grad is not None:
            params.grad.zero_()

        t_p = model_lin(t_u, *params)
        loss = loss_fn(t_p, t_c)
        loss.backward()

        with torch.no_grad():
            params -= lr * params.grad

        if epoch % 500 == 0:
            print('Epoch %d, Loss %f' % (epoch, float(loss)))

    return params

# (B) Training the non-linear model with different rates
# Training 1: SGD, Learning rate = 0.1
training_nonLin(
    epochs = 5000,
    opt = nonLinOpt_SGD,
    params = nonLin_parameters,
    t_u = t_un,
    t_c = t_c
)
# Training 2: SGD Learning rate = 0.01
lr = 0.01
nonLinOpt_SGD = optim.SGD([nonLin_parameters], lr=lr)
training_nonLin(
    epochs = 5000,
    opt = nonLinOpt_SGD,
    params = nonLin_parameters,
    t_u = t_un,
    t_c = t_c
)
# Training 3: SGD, Learning rate = 0.001
lr = 0.001
nonLinOpt_SGD = optim.SGD([nonLin_parameters], lr=lr)
training_nonLin(
    epochs = 5000,
    opt = nonLinOpt_SGD,
    params = nonLin_parameters,
    t_u = t_un,
    t_c = t_c
)
# Training 4: SGD, Learning rate = 0.0001
lr = 0.0001
nonLinOpt_SGD = optim.SGD([nonLin_parameters], lr=lr)
training_nonLin(
    epochs = 5000,
    opt = nonLinOpt_SGD,
    params = nonLin_parameters,
    t_u = t_un,
    t_c = t_c
)
# Training 1: ADAM, Learning rate = 0.1
lr = 0.1
nonLinOpt_ADAM = optim.Adam([nonLin_parameters], lr=lr)
training_nonLin(
    epochs = 5000,
    opt = nonLinOpt_ADAM,
    params = nonLin_parameters,
    t_u = t_un,
    t_c = t_c
)
# Training 2: ADAM, Learning rate = 0.01
lr = 0.01
nonLinOpt_ADAM = optim.Adam([nonLin_parameters], lr=lr)
training_nonLin(
    epochs = 5000,
    opt = nonLinOpt_ADAM,
    params = nonLin_parameters,
    t_u = t_un,
    t_c = t_c
)
# Training 3: ADAM, Learning rate = 0.001
lr = 0.001
nonLinOpt_ADAM = optim.Adam([nonLin_parameters], lr=lr)
training_nonLin(
    epochs = 5000,
    opt = nonLinOpt_ADAM,
    params = nonLin_parameters,
    t_u = t_un,
    t_c = t_c
)
# Training 4: ADAM, Learning rate = 0.0001
lr = 0.0001
nonLinOpt_ADAM = optim.Adam([nonLin_parameters], lr=lr)
training_nonLin(
    epochs = 5000,
    opt = nonLinOpt_ADAM,
    params = nonLin_parameters,
    t_u = t_un,
    t_c = t_c
)
# (C) "Best" non-linear model = ADAM, lr = 0.001
# Run once again to double check params
lr = 0.001
nonLinOpt_ADAM = optim.Adam([nonLin_parameters], lr=lr)
training_nonLin(
    epochs = 5000,
    opt = nonLinOpt_ADAM,
    params = nonLin_parameters,
    t_u = t_un,
    t_c = t_c
)
# Linear model and training
training_lin(
    epochs = 5000,
    lr = 0.1,
    params = lin_parameters,
    t_u = t_un,
    t_c = t_c
)
# Visualizing the data
t_p_non = model_nonLin(t_un, *nonLin_parameters)
t_p_lin = model_lin(t_un, *lin_parameters)

fig = plt.figure()
plt.xlabel("Temperature (°Fahrenheit)")
plt.ylabel("Temperature (°Celsius)")
plt.plot(t_u.numpy(), t_p_non.detach().numpy())
plt.plot(t_u.numpy(), t_p_lin.detach().numpy())
plt.plot(t_u.numpy(), t_c.numpy(), 'o')

#Problem 2 
import torch
import torch.optim as optim
import pandas as pd
from sklearn.preprocessing import StandardScaler

file_path = 'https://raw.githubusercontent.com/HamedTabkhi/Intro-to-ML/refs/heads/main/Dataset/Housing.csv'
dataset = pd.DataFrame(pd.read_csv(file_path))
dataset.head()

# Binary map Outputs to 0 or 1
# M = 1
# B = 0
# Map function
def binary_map(x):
    return x.map({'yes': 1, 'no': 0})

varlist = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']
dataset[varlist] = dataset[varlist].apply(binary_map)
# Remove uneeded information
dataset.pop('furnishingstatus')
dataset.head()

# Standarize the data to remove size differences
stdScaler = StandardScaler()
num_vars = ['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea']
dataset[num_vars] = stdScaler.fit_transform(dataset[num_vars])
# Seperate data into inputs and outputs
x_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']

Y = dataset.pop('price')
X = dataset[x_vars].values
# convert to tensors
X = torch.tensor(X)
Y = torch.tensor(Y)
X, Y

# Split data into training and validation sets
n_samples = X.shape[0]
n_val = int(0.2 * n_samples)

shuffled_indices = torch.randperm(n_samples)

train_indices = shuffled_indices[:-n_val]
val_indices = shuffled_indices[-n_val:]

train_indices, val_indices

# Training set
train_X = X[train_indices]
train_Y = Y[train_indices]
# Validation set
val_X = X[val_indices]
val_Y = Y[val_indices]

# Linear model
weights = ['w1', 'w2', 'w3', 'w4', 'w5']
def model(x, w0, w1, w2, w3, w4, w5) :
    return w0 + x[:,0]*w1 + x[:,1]*w2 + x[:,2]*w3 + x[:,3]*w4 + x[:,4]*w5

# Loss function
def loss_fn(y_p, y) :
    squared_diffs = (y_p - y)**2
    return squared_diffs.mean()

# Training Function
# Training Function with Best Parameters Tracking
def training_fn_with_best_params(epochs, opt, params, x_train, y_train, x_val, y_val):
    best_val_loss = float('inf')
    best_params = None

    for epoch in range(1, epochs + 1):
        train_p = model(x_train, *params)
        train_loss = loss_fn(train_p, y_train)

        val_p = model(x_val, *params)
        val_loss = loss_fn(val_p, y_val)

        opt.zero_grad()
        train_loss.backward()
        opt.step()

        # Track the best parameters based on validation loss
        if val_loss.item() < best_val_loss:
            best_val_loss = val_loss.item()
            best_params = params.clone().detach()

        if epoch % 500 == 0:
            print(f"Epoch {epoch}, Training loss {train_loss.item():.4f},"
                  f" Validation loss {val_loss.item():.4f}")

    print(f"Best Validation Loss: {best_val_loss:.4f}")
    return best_params, best_val_loss

# Modified Training Loop
best_overall_params = None
best_overall_loss = float('inf')

print("Training with SGD:")
for n in training_lr:
    parameters = torch.tensor([1.0, 0.0, 0.0, 0.0, 0.0, 0.0], requires_grad=True)
    optim_SGD = optim.SGD([parameters], lr=n)
    print(f"Learning Rate: {n}")
    best_params, best_loss = training_fn_with_best_params(
        epochs=5000,
        opt=optim_SGD,
        params=parameters,
        x_train=train_X,
        y_train=train_Y,
        x_val=val_X,
        y_val=val_Y
    )
    print(" ")
    if best_loss < best_overall_loss:
        best_overall_loss = best_loss
        best_overall_params = best_params

print("Training with Adam:")
for n in training_lr:
    parameters = torch.tensor([1.0, 0.0, 0.0, 0.0, 0.0, 0.0], requires_grad=True)
    optim_ADAM = optim.Adam([parameters], lr=n)
    print(f"Learning Rate: {n}")
    best_params, best_loss = training_fn_with_best_params(
        epochs=5000,
        opt=optim_ADAM,
        params=parameters,
        x_train=train_X,
        y_train=train_Y,
        x_val=val_X,
        y_val=val_Y
    )
    print(" ")
    if best_loss < best_overall_loss:
        best_overall_loss = best_loss
        best_overall_params = best_params

print("\nBest Overall Parameters:")
print(best_overall_params)
print(f"Best Overall Validation Loss: {best_overall_loss:.4f}")

#Problem 3
import torch
import torch.optim as optim
import pandas as pd
from sklearn.preprocessing import StandardScaler

file_path = 'https://raw.githubusercontent.com/HamedTabkhi/Intro-to-ML/refs/heads/main/Dataset/Housing.csv'
dataset = pd.DataFrame(pd.read_csv(file_path))
dataset.head()

# Binary map Outputs to 0 or 1
# M = 1
# B = 0
# Map function
def binary_map(x):
    return x.map({'yes': 1, 'no': 0})

varlist = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']
dataset[varlist] = dataset[varlist].apply(binary_map)
# Remove uneeded information
dataset.pop('furnishingstatus')
dataset.head()

# Standarize the data to remove size differences
stdScaler = StandardScaler()
num_vars = ['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea']
dataset[num_vars] = stdScaler.fit_transform(dataset[num_vars])
# Seperate data into inputs and outputs

Y = dataset.pop('price')
X = dataset.values
# convert to tensors
X = torch.tensor(X)
Y = torch.tensor(Y)
X, Y

# Split data into training and validation sets
n_samples = X.shape[0]
n_val = int(0.2 * n_samples)

shuffled_indices = torch.randperm(n_samples)

train_indices = shuffled_indices[:-n_val]
val_indices = shuffled_indices[-n_val:]

train_indices, val_indices

# Training set
train_X = X[train_indices]
train_Y = Y[train_indices]
# Validation set
val_X = X[val_indices]
val_Y = Y[val_indices]

# Linear model
#weights = ['w1', 'w2', 'w3', 'w4', 'w5']
def model(x, w0, w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11) :
    return w0 + x[:,0]*w1 + x[:,1]*w2 + x[:,2]*w3 + x[:,3]*w4 + x[:,4]*w5 + x[:,5]*w6 + x[:,6]*w7 + x[:,7]*w8 + x[:,8]*w9 + x[:,9]*w10 + x[:,10]*w11

# Loss function
def loss_fn(y_p, y) :
    squared_diffs = (y_p - y)**2
    return squared_diffs.mean()

# Training Function
# Modified Training Function with Best Parameters Tracking
def training_fn_with_best_params(epochs, opt, params, x_train, y_train, x_val, y_val):
    best_val_loss = float('inf')
    best_params = None

    for epoch in range(1, epochs + 1):
        train_p = model(x_train, *params)
        train_loss = loss_fn(train_p, y_train)

        val_p = model(x_val, *params)
        val_loss = loss_fn(val_p, y_val)

        opt.zero_grad()
        train_loss.backward()
        opt.step()

        # Track the best parameters based on validation loss
        if val_loss.item() < best_val_loss:
            best_val_loss = val_loss.item()
            best_params = params.clone().detach()

        if epoch % 500 == 0:
            print(f"Epoch {epoch}, Training loss {train_loss.item():.4f},"
                  f" Validation loss {val_loss.item():.4f}")

    print(f"Best Validation Loss: {best_val_loss:.4f}")
    return best_params, best_val_loss

# Initialize overall tracking variables
best_overall_params = None
best_overall_loss = float('inf')

print("Training with SGD:")
for n in training_lr:
    parameters = torch.tensor([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], requires_grad=True)
    optim_SGD = optim.SGD([parameters], lr=n)
    print(f"Learning Rate: {n}")
    best_params, best_loss = training_fn_with_best_params(
        epochs=5000,
        opt=optim_SGD,
        params=parameters,
        x_train=train_X,
        y_train=train_Y,
        x_val=val_X,
        y_val=val_Y
    )
    print(" ")
    if best_loss < best_overall_loss:
        best_overall_loss = best_loss
        best_overall_params = best_params

print("Training with Adam:")
for n in training_lr:
    parameters = torch.tensor([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], requires_grad=True)
    optim_ADAM = optim.Adam([parameters], lr=n)
    print(f"Learning Rate: {n}")
    best_params, best_loss = training_fn_with_best_params(
        epochs=5000,
        opt=optim_ADAM,
        params=parameters,
        x_train=train_X,
        y_train=train_Y,
        x_val=val_X,
        y_val=val_Y
    )
    print(" ")
    if best_loss < best_overall_loss:
        best_overall_loss = best_loss
        best_overall_params = best_params

print("\nBest Overall Parameters:")
print(best_overall_params)
print(f"Best Overall Validation Loss: {best_overall_loss:.4f}")
