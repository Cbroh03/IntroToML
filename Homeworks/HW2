import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler

file_path = 'https://raw.githubusercontent.com/HamedTabkhi/Intro-to-ML/refs/heads/main/Dataset/Housing.csv'

housing = pd.DataFrame(pd.read_csv(file_path))

m=len(housing)

varlist = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']
def binary_map(x):
    return x.map({'yes' : 1, 'no' : 0, 'furnished' : 1, 'semi-furnished' : 0.5, 'unfurnished' : 0 })

housing[varlist] = housing[varlist].apply(binary_map)

np.random.seed(0)
df_train,df_test = train_test_split(housing, train_size=0.8, test_size=0.2)

num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price']
df_Newtrain = df_train[num_vars]
df_Newtest = df_test[num_vars]

scaler = MinMaxScaler()
df_Newtrain[num_vars] = scaler.fit_transform(df_Newtrain[num_vars])

y_Normtrain = df_Newtrain.pop('price')
X_Normtrain = df_Newtrain.copy()

Y= y_Normtrain.values

X0 = df_Newtrain.values[:,0]

#######################################################################################
#Problem 1
#######################################################################################

def compute_cost(X, Y, theta, penalty = 0):
    predictions = X.dot(theta)
    errors = predictions - Y
    sqrErrors = np.square(errors)
    if (penalty == 0):
        J = 1 / (2 * m) * np.sum(sqrErrors)
    else:
        J = 1/(2*m) * (np.sum(sqrErrors) + penalty * (np.sum(theta) - theta[0]))
    return J

def DisplayData(X, color):
    plt.scatter(X,Y,color = color)
    plt.grid()
    plt.title('Scatter plot of Price of Home')
    plt.rcParams["figure.figsize"] = (10,6)
    plt.xlabel('area')
    plt.ylabel('price')

def gradient(cost_history, color):
    plt.plot(range(1,iterations+1),cost_history,color = color)
    plt.rcParams["figure.figsize"] = (10,6)
    plt.grid()
    plt.xlabel('Number of iterations')
    plt.ylabel('Cost')
    plt.title('Convergence of gradient descent')

def gradient_descent(X, Y, X2, Y2, theta, alpha, iterations, penalty = 0):
    cost_history = np.zeros(iterations)
    cost_history2 = np.zeros(iterations)
    for i in range(iterations):
        prediction = X.dot(theta)
        errors = np.subtract(prediction,Y)
        sum_delta = (alpha/m)*X.transpose().dot(errors);
        if (penalty == 0):
            theta = theta - sum_delta;
        else:
            theta = theta * (1 - alpha * (penalty / m)) - sum_delta;

        cost_history[i] = compute_cost(X,Y,theta, penalty)
        cost_history2[i] = compute_cost(X2, Y2, theta)

    return theta, cost_history, cost_history2

def multiGraph(size, X, Y, X2, Y2, alpha = 0.01, penalty = 0):
    X0 = np.ones((len(X), 1))
    X = np.hstack((X0, X))

    X0 = np.ones((len(X2), 1))
    X2 = np.hstack((X0, X2))

    theta = np.zeros(size)
    iterations = 1500;

    cost = compute_cost(X, Y, theta)
    theta, cost_history, cost_history2 = gradient_descent(X, Y, X2, Y2, theta, alpha, iterations, penalty)

    return theta, cost_history, cost_history2

raw_data = pd.DataFrame(pd.read_csv(file_path))

m = len(raw_data)
raw_data[varlist] = raw_data[varlist].apply(binary_map)

#Used to split the training and test
from sklearn.model_selection import train_test_split
np.random.seed(0)
train_data, test_data = train_test_split(raw_data, train_size=0.8, test_size = 0.2, random_state = 42)

def preprocessing(inputvars, data, Select = 'False'):
    data = data[inputvars]
    if Select == 'normalization':
        scaler = MinMaxScaler()
        data[inputvars] = scaler.fit_transform(data[inputvars])
    if Select == 'standardization':
        scaler = StandardScaler()
        data[inputvars] = scaler.fit_transform(data[inputvars])

    Y = data.pop('price')
    X = data

    return X, Y

def Everything(inputvars, Norm, alpha = 0.01, penalty = 0):
    X_train, Y_train = preprocessing(inputvars, train_data, Norm)
    X_test, Y_test = preprocessing(inputvars, test_data, Norm)

    theta_Train, cost_history_Train, cost_history_Test = multiGraph(len(inputvars), X_train, Y_train, X_test, Y_test, alpha, penalty)
    plt.rcParams["figure.figsize"] = (10,6)
    plt.grid()
    gradient(cost_history_Train, 'red')
    gradient(cost_history_Test, 'blue')

    return theta_Train

input1 = ['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'parking']
input2 = ['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea']
iterations = 1500

#1a
plt.clf()
theta = Everything(input1, 'False', 0.0000000002)
plt.title('Problem 1a')
print('below is the best paramaters for 1a')
print('\n'.join('{}: {}'.format(*val) for val in enumerate(theta)))
plt.legend(['Training','Validation'])
plt.show()


#1b
plt.clf()
theta = Everything(input2, 'False', 0.0000000002)
plt.title('Problem 1b')
print('below is the best paramaters for 1b')
print('\n'.join('{}: {}'.format(*val) for val in enumerate(theta)))
plt.legend(['Training','Validation'])
plt.show()


#######################################################################################
#Problem 2
#######################################################################################

#2a
plt.clf()
theta_Train = Everything(input1, 'normalization', 0.01)
plt.title('Problem 2a')
print('below is the best paramaters for 2a')
print('\n'.join('{}: {}'.format(*val) for val in enumerate(theta)))
plt.legend(['Training','Validation'])
plt.show()

#2b
plt.clf()
theta_Train = Everything(input2, 'standardization')
plt.title('Problem 2b')
print('below is the best paramaters for 2b')
print('\n'.join('{}: {}'.format(*val) for val in enumerate(theta)))
plt.legend(['Training','Validation'])
plt.show()

#######################################################################################
#Problem 3
#######################################################################################

#3a
plt.clf()
theta = Everything(input1, 'normalization', 0.01, 3)
plt.title('Problem 3a')
print('below is the best paramaters for 3a')
print('\n'.join('{}: {}'.format(*val) for val in enumerate(theta)))
plt.legend(['Training','Validation'])
plt.show()

#3b
plt.clf()
theta = Everything(input2, 'standardization', 0.01, 3)
plt.title('Problem 3b')
print('below is the best paramaters for 3b')
print('\n'.join('{}: {}'.format(*val) for val in enumerate(theta)))
plt.legend(['Training','Validation'])
plt.show()
