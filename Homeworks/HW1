import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

file_path = 'https://raw.githubusercontent.com/HamedTabkhi/Intro-to-ML/main/Dataset/D3.csv'
df = pd.read_csv(file_path)
x1 = df.values[:, 0]
x2 = df.values[:, 1]
x3 = df.values[:, 2]
y = df.values[:, 3]
m = len(y)

def compute_cost(X, y, theta):  #Cost function for doing predictions
    m = len(y)
    predictions = X.dot(theta)
    cost = (1 / (2 * m)) * np.sum(np.square(predictions - y))
    return cost

def gradient_descent(X, y, theta, alpha, iterations):  #Gradient decent function to
    m = len(y)
    cost_history = np.zeros(iterations)
    for i in range(iterations):
        predictions = X.dot(theta)
        theta = theta - (1 / m) * alpha * (X.T.dot(predictions - y))
        cost_history[i] = compute_cost(X, y, theta)
    return theta, cost_history

def run_linear_regression(x, y, alpha, iterations):  #Runs the linear regression for each input variable
    X = np.c_[np.ones(m), x]  #Add a column of ones for the intercept term
    theta = np.zeros(2)  #Initialize parameters to zero
    final_theta, cost_history = gradient_descent(X, y, theta, alpha, iterations)
    return final_theta, cost_history

def plot_regression_line(x, y, theta):  #Plots linar regression
    plt.scatter(x, y, color='blue', marker='x', label='Data points')
    plt.plot(x, theta[0] + theta[1] * x, color='red', label='Regression line')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.legend()
    plt.show()

def plot_loss(cost_history):  #Plots loss function over num iterations
    plt.plot(cost_history)
    plt.xlabel('Iterations')
    plt.ylabel('Cost (Loss)')
    plt.title('Cost function over iterations')
    plt.show()

alpha = 0.015  #Learning rate
iterations = 1500  #Number of iterations

#Training each var. individually
theta_x1, cost_history_x1 = run_linear_regression(x1, y, alpha, iterations)
print(f"Theta for x1: {theta_x1}")
plot_regression_line(x1, y, theta_x1)
plot_loss(cost_history_x1)

theta_x2, cost_history_x2 = run_linear_regression(x2, y, alpha, iterations)
print(f"Theta for x2: {theta_x2}")
plot_regression_line(x2, y, theta_x2)
plot_loss(cost_history_x2)

theta_x3, cost_history_x3 = run_linear_regression(x3, y, alpha, iterations)
print(f"Theta for x3: {theta_x3}")
plot_regression_line(x3, y, theta_x3)
plot_loss(cost_history_x3)

print(f"Final loss value for x1: {cost_history_x1[-1]}")
print(f"Final loss value for x2: {cost_history_x2[-1]}")
print(f"Final loss value for x3: {cost_history_x3[-1]}")

# ----------------------------- Problem 2: Multi-variable Gradient Descent -----------------------------

def run_multi_var_regression(X, y, alpha, iterations):  #New regression function to run with multi variables
    X = np.c_[np.ones(m), X]  #Add a column of ones for the intercept term
    theta = np.zeros(X.shape[1])  #Initialize parameters to zero
    final_theta, cost_history = gradient_descent(X, y, theta, alpha, iterations)
    return final_theta, cost_history

X_combined = np.column_stack((x1, x2, x3))  #Stacking X1, X2, and X3 into one matrix

theta_multi, cost_history_multi = run_multi_var_regression(X_combined, y, alpha, iterations)
print(f"Theta for multi-variable regression: {theta_multi}")

plot_loss(cost_history_multi)

def predict(X, theta):  #Predictions for the new input values
    X = np.c_[np.ones(X.shape[0]), X]  # Add a column of ones for the intercept term
    return X.dot(theta)

new_values = np.array([[1, 1, 1], [2, 0, 4], [3, 2, 1]])  # New input values
predictions = predict(new_values, theta_multi)
print(f"Predictions for new values (1,1,1), (2,0,4), (3,2,1): {predictions}")
